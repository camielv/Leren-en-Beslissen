\documentclass[11pt]{article}
\usepackage[a4paper,pdftex]{geometry}
\setlength{\oddsidemargin}{5mm}
\setlength{\evensidemargin}{5mm}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{url}
\usepackage{qtree}
\usepackage{lastpage}
\usepackage{multirow}
\urlstyle{same}

% Page numbering
\lhead{Sentiment Analysis - A Probabilistic Approach}
\rhead{page \thepage/\pageref{LastPage}}
\cfoot{}
\rfoot{\thepage}

% TITLE FORMAT
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\makeatletter
\def\printtitle{
    {\centering \@title\par}}
\makeatother									

\makeatletter
\def\printauthor{
    {\centering \large \@author}}
\makeatother

% TITLE
\title{
\HRule{0.5pt} \\
\LARGE \textbf{\textsc{Sentiment Analysis}}\\[0.5cm]
\normalsize \textsc{A Probabilistic Approach}
\HRule{2pt}\\ [0.5cm]
\normalsize
\today\\ [4cm]
\includegraphics[width=0.6\textwidth]{titel.png}\\
}

\author{
Supervised by I. Langbroek (Blauw Research)\\
Mentored by dr. M. van Someren (Universiteit van Amsterdam)\\[0.5cm]
\begin{tabular}{c c c c c}
S. A. Gieske & S. Laan & C. R. Verschoor & D. S. ten Velthuis & A. J. Wiggers\\
6167667 & 6036031 & 10017321 & 0577642 & 6036163
\end{tabular}\\[0.5cm]
Artificial Intelligence\\
Faculty of Science\\
Universiteit van Amsterdam\\
}

% BEGIN DOCUMENT
\begin{document}

% TITLE PAGE
\thispagestyle{empty}
\printtitle									
\vfill
\printauthor
\newpage

% TABLE OF CONTENTS
\setcounter{page}{1}
\normalsize
\tableofcontents
\newpage

% CONTENTS

%Net maarten gevraagd doel van dity verslag is om de opdracht gever zijn vraag te beantwoorden en hoe we dat hebben gedaan
%geen academische paper alleen uitleg van theory die hij niet kent en hoe we probleem hebben aangepakt en waarom we dat zo hebben gedaan

\section{Introduction} 
With the growth of Social Media, such as Facebook, Twitter and blogs, consumers gained a place to review, rate and recommend products online. This online opinion is important for companies who want to market their products, manage their reputations and identify new opportunities. The process of finding relevant content, filtering out noise and categorization of messages can be automated, making it several times faster than manual processing.

Sentiment analysis is the application of natural language processing, computational linguistics and text analytics to identify and extract subjective information in source materials. It readies data from social media for further analysis. Through use of sentiment analysis, messages can be clustered into categories (e.g. positive, negative and neutral messages). 

\section{Problem Specification}
The dataset was provided by our client, a spreadsheet of 10.000 messages from social media\footnote{ Data contained posts from blogs, facebook, hyves, fora and tweets}. All messages were manually placed into 5 categories, ranging from very negative to very positive. The goal of this project is to create a method to classify these messages automatically.

% By loading the .csv version of the excel file we where able to access the data in Python. 

\subsection{Data Cleaning}
A tweet can only contain 140 characters. To still be able to express oneself, slang and emoticons are used. It is also common to find spelling errors in the messages. This increases the value of cleaning the messages before classifying them.
Punctuation usually does not add to the sentiment of the message. Therefore, all punctuation except exclamation marks and question marks are ignored. These two do give value to the sentiment of a sentence because it says something about the emotion of the user. 

A Dutch stemmer, found in the natural language processing package for Python, enables reducing inflected words to their stem, base or root form. Two words that lead to the same classification, for example "Evangelisch" and "Evangelische", are related to each other. By stemming the last word to its base form "Evangelisch", the amount of different words in the corpus is decreased. This makes classification much easier, AS WILL BE SHOWN IN WAAAR!???

\subsection{Data Reduction}
The dataset contains mostly tweets. These short messages usually contain a single subject, whereas blog and facebook posts are in general longer messages that may contain several opinions about several subjects. Twitter being the largest source of information, it was decided that only these posts would be used for classification. 

There are several methods to further decrease the vast amount of data:
\begin{itemize}
\item Remove words that occur only once in the corpus.
\item Remove duplicate messages
\item Remove words that do not contribute to classification (personal pronouns, so-called stopwords, etc.)
\end{itemize}
Which of these methods would be useful depends on the machine learning method, e.g. an approach that considers grammar to be an important feature for classification makes it impossible to remove words from a sentence without affecting the results.

\section{Theory}
%Neural Network, Naive Bayes, MaxEnt, Perceptron
\subsection{Relevant Literature}

\subsection{Machine Learning} 
Machine Learning is a branch in the field of artifical intellgence that researches and develops algorithms cabable of improving predictions or behaviors based on a dataset. An algorithm can take example data to capture the pattern in the underlying probability distribution. A succesful algorithm can automatically find these complex patterns in the training data and make intelligent desisions in new (similar) data. The type of machine learning methods that are used in this assignment is called `supervised learning'. This type requires that the desired decision, in this case the sentiment of the messages, is already known.  

%tod do: uitleg over verschillende soort machien learning (supervised unsupervised reinforcemnt etc)


\subsubsection{Perceptron}

A perceptron is an example of a supervised learning algorithm. It is an artificial neuron that has one or more inputs $s_i$ with corresponding weights $w_i$, a threshold function $t$ and an output $o$. This output can be defined as $o = t(\sum_i(w_i * s_i))$. The algorithm that enables `learning' a correct threshold goes through these steps:
\begin{enumerate}
\item Select training data from the dataset: input and it's corresponding (correct) output
\item Feed the perceptron the input
\item Calculate the output
\item Compare the output to the desired output
\item Adjust weights and go to step 2 until satisfied
\end{enumerate}

This algorithm adjusts the weights $w_i$, trying to maximize the number of correct outcomes. It should be noted that a perceptron has only a single output, making it hard to perform classification when there is more than one class. 

\subsubsection{Artificial Neural Network}
 An artificial neural network is a computational model with the same functional aspects of biological neural networks. It consists of a group of connected artificial neurons that changes its structure based on internal and external information. These neurons are similar to perceptrons, the only difference being that an output can be the input for another neuron. A typical network consists of an input layer, one or more so-called hidden layers and an output layer. The number of nodes per layer can vary. The learning process is very similar to that of a perceptron but is slightly more difficult since the weights are not independent.

\subsubsection{Maximum Entropy}

\subsubsection{Naive Bayes Classifier}

\subsubsection{Weighted Probability Sum}
This method is a variation on the classic Naive Bayes classifier in that it calculates the probability for a sentence to belong to a certain class by the sum of its elements, e.g. the probability that the sentence `Ik hou van de eo' contains a positive sentiment can be defined as the sum of the probabilities of the words `Ik', `hou', `van', `de' and `eo' to contain positive sentiment. Mathematically, this looks like:
$\displaystyle P(s, C) = \sum_{i=0}^n P( s[i], C )$, where $C$ is the class, $s $ is the sentence and $s[i]$ is the $i^{th}$ element of s. 

This approach uses each word in the sentence as a feature. It is however possible to take multiple words as a feature. A sentence of length $l$ where $l \leq n$ words contains $l - (n-1)$ sequences of $n$ words. Negations can be included for $n \leq 2$, double negations for $n \leq 3$, etc. 


\subsection{Classification Measures}
%Waarschijnlijk deze substecion voor machine learning

In order to test how well a specific algorithm performs, a couple of measurements are used. Each will be explained briefly in terms of table \ref{classification}.

\begin{table}[h]
\center
\begin{tabular}{cc|c|c|}
\cline{3-4}
& & \multicolumn{2}{|c|}{Classified Class} \\ \cline{3-4}
& &  Positive & Negative \\ \cline{1-4}
\multicolumn{1}{|c|}{\multirow{2}{*}{Actual Class}} &
\multicolumn{1}{|c|}{Positive} &  TP & FN      \\ \cline{2-4}
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{Negative} & FP & TN     \\ \cline{1-4}
\end{tabular}
\caption{Table of classification classes}
\label{classification}
\end{table} 


\subsubsection{Accuracy}

\subsubsection{Precision}

The precision is defined as follows:

\begin{equation*}
\textit{Precision} = \frac{ \textit{TP}}{\textit{TP} + \textit{FP}}
\end{equation*}

\subsubsection{Recall}

\subsubsection{F-Measure}



\section{Usage and User Guide}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\section{Implementation and Results}


\subsection{Global Approach}
the general approach will be explained first. The idea is to use a binary classifier, i.e.\ either true or false, multiple times. This way it is possible to first decide whether a message is neutral and then decide between the non-neutral messages whether it is a positive or negative message. Maybe a tree structure shows the used approach more clearly:
\begin{figure}[h]
\Tree [.{All messages} {Neutral Messages} [.{Non-Neutral Messages} {Positive Messages} {Negative Messages} ] ]
\end{figure}

As you can see, using this approach you have to distinguish between two classes at a time. This is very convenient, because there are a lot of techniques that can discriminate between two classes, whereas multi-class-classifiers are less common.

\subsection{Binary Classification}
In this section, the method that we use to classify a message is described. The approach consists of a few steps:
\begin{enumerate}
\item The counting of word frequencies
\item Calculation of word probabilities
\item Calculation of message probabilities
\item Finding a good threshold
\end{enumerate}

First the frequency of each word, i.e. we count how many times it occurs in the training set, is counted. The number  The number of times the word occurs in the opposite class, is equivalent to the total number of encounters, minus the encounters in the first class.

When the frequencies of all words\footnote{All words in the training set} are found, the probability for each word can be calculated. This gives an idea of how likely it is to be encountered in a certain class. The formula for this probability is the following:

\begin{equation}
P(word) = \frac{ \sum word \in C_1}{\sum word \in C_1\cup C_2}
\end{equation}

So the probability that a word is in $C_1$, the first class, is the number of times it has been encountered in a sentence, which was tagged to be in $C_1$, divided by the total number of encounters.

Now that all words have probabilities assigned to them, or at least all words in the training set, the probabilities of the sentences can be calculated.

\begin{equation}
P(s) = \frac{1}{n} \sum_{w \in s} P(w)
\end{equation}

Where $s$ is the sentence, $n$ the number of words in the sentence and $w$ a word in the sentence. As follows from the formula,  the weighted sum of the word probabilities is used.

Now that every sentence has an associated probability, the real machine learning can begin. The goal is to find a threshold for the probabilities; when a sentence probability is higher than a certain amount, it is classified it as $C_1$,  otherwise it belongs to $C_2$.

\subsection{Algoritme 1}
\subsubsection{Approach}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\subsubsection{Results}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\subsubsection{Discussion}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\subsection{Algoritme 2}
\subsubsection{Approach}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\subsubsection{Results}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\subsubsection{Discussion}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.



\section{Discussion}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

\section{Conclusion}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis vulputate molestie mi ac dignissim. Proin tristique convallis volutpat. Nunc semper erat id tortor fermentum ullamcorper. Donec sed erat quis erat condimentum pellentesque. Donec sed tristique quam. Proin dictum convallis velit a porttitor. Curabitur in tellus tortor. Proin aliquet blandit sagittis. Curabitur vitae mauris ac leo dignissim rhoncus nec ut orci. Praesent vulputate mollis auctor. Aenean in felis diam, quis dictum metus.


\end{document}
% END DOCUMENT